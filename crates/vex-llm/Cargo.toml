[package]
name = "vex-llm"
version.workspace = true
edition.workspace = true
license.workspace = true
repository.workspace = true
description = "LLM provider integrations for VEX"
readme = "README.md"
keywords = ["ai", "llm", "openai", "ollama", "agents"]
categories = ["api-bindings", "asynchronous"]

[dependencies]
vex-core = { workspace = true }
tokio = { workspace = true }
async-trait = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
thiserror = { workspace = true }
tracing = { workspace = true }
reqwest = { version = "0.12", features = ["json"] }
chrono = { workspace = true }
uuid = { version = "1", features = ["v4"] }
rand = { workspace = true }
meval = "0.2"
futures = "0.3"
sha2 = { workspace = true }
hex = { workspace = true }
regex = "1"
async-stream = "0.3"
tokio-stream = "0.1"
pin-project-lite = "0.2"
jsonschema = { workspace = true }
tokio-tungstenite = "0.24"

# High-performance caching
moka = { version = "0.12", features = ["future"] }

[dev-dependencies]
vex-macros = { workspace = true }

[features]
default = []
openai = ["dep:async-openai"]

[dependencies.async-openai]
version = "0.24"
optional = true
